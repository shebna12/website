{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Full Stack Deep Learning Our mission is to help you go from a promising ML experiment to a shipped product, with real-world impact. Current Course We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course. As always, we are posting all materials online. Go to Course About this course There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale Who is this for The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review. Instructors Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp Previous Course Offerings Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Home"},{"location":"#current-course","text":"We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course. As always, we are posting all materials online. Go to Course","title":"Current Course"},{"location":"#about-this-course","text":"There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale","title":"About this course"},{"location":"#who-is-this-for","text":"The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review.","title":"Who is this for"},{"location":"#instructors","text":"Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp","title":"Instructors"},{"location":"#previous-course-offerings","text":"Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Previous Course Offerings"},{"location":"spring2021/","text":"Full Stack Deep Learning - Spring 2021 We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details . Week 1: Fundamentals The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Week 2: CNNs The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Week 3: RNNs The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs and NLP Applications Lab 3: RNNs Week 4: Transformers The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Week 5: ML Projects The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects Those in the syncronous online course will have their first weekly assignment: Assignment 1, available on Gradescope. Week 6: Infra & Tooling The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. In lab, we learn to manage experiments. Lecture 6: Infrastructure & Tooling Lab 5: Experiment Management Those in the syncronous online course will have to work on Assignment 2. Week 7: Troubleshooting The week of March 14, we talk about how to best troubleshoot training. In lab, we move from lines to paragraphs. Lecture 7: Troubleshooting DNNs Lab 6: Line Detection or Paragraph Recognition Those in the syncronous online course will have to work on Assignment 3. Week 8: Data The week of March 21, we talk about Data Management. Lecture 8: Data Management Lab 7: Data Management Those in the syncronous online course will have to work on Assignment 4. Week 9: Ethics The week of March 28, we discuss ethical considerations. Lecture 9: AI Ethics Those in the synchronous online course will have to submit their project proposals. Week 10: Testing The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects. Week 11: Deployment The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects. Week 12: Research The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects. Week 13: Teams The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups. Week 14: Projects The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too...","title":"Spring 2021 Schedule"},{"location":"spring2021/#full-stack-deep-learning-spring-2021","text":"We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details .","title":"Full Stack Deep Learning - Spring 2021"},{"location":"spring2021/#week-1-fundamentals","text":"The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro","title":"Week 1: Fundamentals"},{"location":"spring2021/#week-2-cnns","text":"The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs","title":"Week 2: CNNs"},{"location":"spring2021/#week-3-rnns","text":"The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs and NLP Applications Lab 3: RNNs","title":"Week 3: RNNs"},{"location":"spring2021/#week-4-transformers","text":"The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers","title":"Week 4: Transformers"},{"location":"spring2021/#week-5-ml-projects","text":"The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects Those in the syncronous online course will have their first weekly assignment: Assignment 1, available on Gradescope.","title":"Week 5: ML Projects"},{"location":"spring2021/#week-6-infra-tooling","text":"The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. In lab, we learn to manage experiments. Lecture 6: Infrastructure & Tooling Lab 5: Experiment Management Those in the syncronous online course will have to work on Assignment 2.","title":"Week 6: Infra &amp; Tooling"},{"location":"spring2021/#week-7-troubleshooting","text":"The week of March 14, we talk about how to best troubleshoot training. In lab, we move from lines to paragraphs. Lecture 7: Troubleshooting DNNs Lab 6: Line Detection or Paragraph Recognition Those in the syncronous online course will have to work on Assignment 3.","title":"Week 7: Troubleshooting"},{"location":"spring2021/#week-8-data","text":"The week of March 21, we talk about Data Management. Lecture 8: Data Management Lab 7: Data Management Those in the syncronous online course will have to work on Assignment 4.","title":"Week 8: Data"},{"location":"spring2021/#week-9-ethics","text":"The week of March 28, we discuss ethical considerations. Lecture 9: AI Ethics Those in the synchronous online course will have to submit their project proposals.","title":"Week 9: Ethics"},{"location":"spring2021/#week-10-testing","text":"The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects.","title":"Week 10: Testing"},{"location":"spring2021/#week-11-deployment","text":"The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects.","title":"Week 11: Deployment"},{"location":"spring2021/#week-12-research","text":"The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects.","title":"Week 12: Research"},{"location":"spring2021/#week-13-teams","text":"The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups.","title":"Week 13: Teams"},{"location":"spring2021/#week-14-projects","text":"The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too...","title":"Week 14: Projects"},{"location":"spring2021/lab-1/","text":"Lab 1: Setup and Introduction Video In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST Slides PDF Download Follow Along GitHub Readme","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#lab-1-setup-and-introduction","text":"","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#video","text":"In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST","title":"Video"},{"location":"spring2021/lab-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lab-1/#follow-along","text":"GitHub Readme","title":"Follow Along"},{"location":"spring2021/lab-2/","text":"Lab 2: CNNs and Synthetic Data Video No slides. Follow Along GitHub Readme","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#lab-2-cnns-and-synthetic-data","text":"","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#video","text":"No slides.","title":"Video"},{"location":"spring2021/lab-2/#follow-along","text":"GitHub Readme","title":"Follow Along"},{"location":"spring2021/lab-3/","text":"Lab 3: RNNs Video No slides Follow along Readme Notes","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#lab-3-rnns","text":"","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-3/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-3/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-1/","text":"Lecture 1: DL Fundamentals Video Slides PDF Download Notes In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#lecture-1-dl-fundamentals","text":"","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#video","text":"","title":"Video"},{"location":"spring2021/lecture-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-1/#notes","text":"In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Notes"},{"location":"spring2021/lecture-2a/","text":"Lecture 2A: CNNs Video Slides PDF Download Notes TODO","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#lecture-2a-cnns","text":"","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2a/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2a/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lecture-2b/","text":"Lecture 2B: Computer Vision Video Slides PDF Download Notes TODO","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#lecture-2b-computer-vision","text":"","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2b/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2b/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lecture-3/","text":"Lecture 3: RNNs Video Slides PDF Download Notes","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#lecture-3-rnns","text":"","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#video","text":"","title":"Video"},{"location":"spring2021/lecture-3/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-3/#notes","text":"","title":"Notes"},{"location":"spring2021/notebook-1/","text":"Notebook: Coding a neural net Video In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron Follow Along Google Colab","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#notebook-coding-a-neural-net","text":"","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#video","text":"In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron","title":"Video"},{"location":"spring2021/notebook-1/#follow-along","text":"Google Colab","title":"Follow Along"},{"location":"spring2021/synchronous/","text":"Synchronous Online Course For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion How do I know if I am in this course? If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack. Teaching Assistants This course is only possible with the support of our amazing TAs: James Le runs Data Relations for Superb AI and contributes Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, a SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Before he joined the dark side, he co-managed TEFQX . Schedule While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project. Logistics All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own. Projects The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. Certificate Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn. Time Commitment On average, expect to spend 5-10 hours per week on the course.","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#synchronous-online-course","text":"For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#how-do-i-know-if-i-am-in-this-course","text":"If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack.","title":"How do I know if I am in this course?"},{"location":"spring2021/synchronous/#teaching-assistants","text":"This course is only possible with the support of our amazing TAs: James Le runs Data Relations for Superb AI and contributes Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, a SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Before he joined the dark side, he co-managed TEFQX .","title":"Teaching Assistants"},{"location":"spring2021/synchronous/#schedule","text":"While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project.","title":"Schedule"},{"location":"spring2021/synchronous/#logistics","text":"All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own.","title":"Logistics"},{"location":"spring2021/synchronous/#projects","text":"The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning.","title":"Projects"},{"location":"spring2021/synchronous/#certificate","text":"Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn.","title":"Certificate"},{"location":"spring2021/synchronous/#time-commitment","text":"On average, expect to spend 5-10 hours per week on the course.","title":"Time Commitment"}]}